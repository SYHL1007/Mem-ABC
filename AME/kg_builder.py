#kg_bulider.py
import logging
import asyncio
import uuid
import json
from typing import List, Dict, Tuple
from schemas import Node, Relation
import schemas
from llm_service import LLMService
from embedding_service import EmbeddingService
from graph_db import GraphDB
import config

logger = logging.getLogger(__name__)

class KnowledgeGraphBuilder:
    def __init__(self, llm: LLMService, embed: EmbeddingService, db: GraphDB):
        self.llm = llm
        self.embed = embed
        self.db = db
    
    def _clean_for_logging(self, data: dict) -> dict:
        """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ·±åº¦å¤åˆ¶ä¸€ä¸ªå­—å…¸å¹¶ç§»é™¤æ‰€æœ‰åµŒå…¥å‘é‡ä»¥ä¾›æ—¥å¿—è®°å½•ã€‚"""
        # ä½¿ç”¨æ·±æ‹·è´é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        from copy import deepcopy
        cleaned_data = deepcopy(data)
        
        # ç§»é™¤é¡¶å±‚çš„åµŒå…¥
        cleaned_data.pop('name_embedding', None)
        cleaned_data.pop('fact_embedding', None)
        
        # æ£€æŸ¥ 'properties' å­—æ®µï¼ˆé’ˆå¯¹ Pydantic æ¨¡å‹ dump å’Œå…³ç³»å­—å…¸ï¼‰
        if 'properties' in cleaned_data and isinstance(cleaned_data['properties'], dict):
            cleaned_data['properties'].pop('name_embedding', None)
            cleaned_data['properties'].pop('fact_embedding', None)
            
        return cleaned_data

    async def process_memory(self, user_id: str, memory_id: str, memory_text: str):
        """
        å¤„ç†å•æ¡è®°å¿†ï¼Œå®Œæˆ èŠ‚ç‚¹æŠ½å–->åµŒå…¥->å»é‡->å…³ç³»æŠ½å–->åµŒå…¥->å»é‡->å­˜å‚¨ çš„å®Œæ•´æµç¨‹ã€‚
        """
        logger.debug(f"[User: {user_id}, Mem: {memory_id}] å¼€å§‹å¤„ç†...")
        
        try:
            # 1. èŠ‚ç‚¹æŠ½å–
            node_response = await self.llm.extract_nodes(memory_text)
            await asyncio.sleep(1.0)
            if not node_response or not node_response.nodes:
                logger.warning(f"[User: {user_id}, Mem: {memory_id}] LLM æœªæŠ½å–åˆ°èŠ‚ç‚¹")
                return
            
            # 2. èŠ‚ç‚¹åµŒå…¥ä¸å»é‡ (æ ¸å¿ƒé€»è¾‘)
            # temp_id -> persistent_uuid çš„æ˜ å°„
            temp_id_map, nodes_to_save = await self._resolve_nodes(
                user_id, memory_id, memory_text, node_response.nodes
            )
            
            if not temp_id_map:
                logger.warning(f"[User: {user_id}, Mem: {memory_id}] èŠ‚ç‚¹è§£æ/å»é‡åæ— æœ‰æ•ˆèŠ‚ç‚¹")
                return

            # 3. å…³ç³»æŠ½å–
            # å°†å¸¦æœ‰æŒä¹…åŒ–UUIDçš„èŠ‚ç‚¹åˆ—è¡¨ä¼ ç»™LLMï¼Œä»¥ä¾¿å®ƒä½¿ç”¨ 'temp_id'
            nodes_for_llm = [
                {
                "temp_id": n.temp_id,
                "label": n.label,
                "properties": {"name": n.properties.name}
                }
                for n in node_response.nodes
                ]
            relation_response = await self.llm.extract_relations(
                memory_text, json.dumps(nodes_for_llm)
            )
            
            if not relation_response or not relation_response.relations:
                logger.debug(f"[User: {user_id}, Mem: {memory_id}] LLM æœªæŠ½å–åˆ°å…³ç³»")
                # å³ä½¿æ²¡æœ‰å…³ç³»ï¼Œæˆ‘ä»¬ä»ç„¶ä¿å­˜èŠ‚ç‚¹
                await self.db.batch_save_graph(user_id, nodes_to_save, [])
                return
                
            # 4. å…³ç³»æ˜ å°„ã€åµŒå…¥ä¸å»é‡
            relations_to_save = await self._resolve_relations(
                user_id, memory_id, memory_text, relation_response.relations, temp_id_map, nodes_to_save
            )
            
            # 5. æ‰¹é‡å­˜å…¥æ•°æ®åº“
            await self.db.batch_save_graph(user_id, nodes_to_save, relations_to_save)
            
            logger.debug(f"[User: {user_id}, Mem: {memory_id}] å¤„ç†å®Œæˆã€‚")

        except Exception as e:
            logger.error(f"[User: {user_id}, Mem: {memory_id}] å¤„ç†å¤±è´¥: {e}", exc_info=True)


    async def _resolve_nodes(
        self, user_id: str, memory_id: str, memory_text: str, nodes_from_llm: List[Node]
    ) -> Tuple[Dict[str, str], List[Node]]:
        """
        å¯¹LLMæŠ½å–çš„èŠ‚ç‚¹è¿›è¡ŒåµŒå…¥å’Œå»é‡ã€‚
        è¿”å›: (temp_id åˆ° persistent_uuid çš„æ˜ å°„, å‡†å¤‡å­˜å…¥DBçš„Nodeåˆ—è¡¨)
        """
        temp_id_map = {}
        nodes_to_save = []
        
        # 1. æå–åç§°å¹¶æ‰¹é‡åµŒå…¥
        names_to_embed = []
        nodes_to_process = []
        
        for node in nodes_from_llm:
            if node.temp_id == "user":
                node.persistent_uuid = user_id 
                node.label = "User" 
                temp_id_map[node.temp_id] = user_id
                nodes_to_save.append(node)
            else:
                names_to_embed.append(node.properties.name)
                # å°† user_id æ³¨å…¥
                node.properties.user_id = user_id
                nodes_to_process.append(node)

        if not nodes_to_process:
            return temp_id_map, nodes_to_save
            
        embeddings = self.embed.embed_batch(names_to_embed)
        
        # 2. å¼‚æ­¥è§£ææ¯ä¸ªèŠ‚ç‚¹
        tasks = []
        for node, embedding in zip(nodes_to_process, embeddings):
            node.properties.name_embedding = embedding
            node.properties.source_memory_id = memory_id
            tasks.append(self._find_or_create_node(user_id, memory_text, node))
            
        resolved_nodes: List[Node] = await asyncio.gather(*tasks)
        
        # 3. æ„å»ºæ˜ å°„
        final_nodes_to_save = {n.persistent_uuid: n for n in nodes_to_save} # å…ˆæŠŠ user èŠ‚ç‚¹åŠ è¿›å»
        for llm_node, resolved_node in zip(nodes_to_process, resolved_nodes):
            if resolved_node.persistent_uuid:
                temp_id_map[llm_node.temp_id] = resolved_node.persistent_uuid
                if resolved_node.persistent_uuid not in final_nodes_to_save:
                    final_nodes_to_save[resolved_node.persistent_uuid] = resolved_node
                
        return temp_id_map, list(final_nodes_to_save.values())

    async def _find_or_create_node(self, user_id: str, memory_text: str, node: Node) -> Node:
        """
        [æœ€ç»ˆä¿®å¤ç‰ˆ] æ‰§è¡Œå¤šå€™é€‰é¡¹çš„â€œåŒé˜ˆå€¼â€å»é‡é€»è¾‘ã€‚
        åœ¨åˆå¹¶æ—¶ï¼ŒåŒæ—¶ç»Ÿä¸€UUIDå’ŒèŠ‚ç‚¹åç§°ã€‚
        """
        similar_nodes = await self.db.find_similar_nodes(
            user_id, node.properties.name_embedding, node.label.strip(':'), top_k=5
        )
        
        logger.debug(f"--- ğŸ•µï¸ Node Dedupe Check for: '{node.properties.name}' ---")

        if not similar_nodes:
            node.persistent_uuid = str(uuid.uuid4())
            logger.debug(f"  - Verdict: NEW (No similar nodes found)")
            return node

        logger.debug(f"  - Candidate: {json.dumps(self._clean_for_logging(node.model_dump()), indent=2, ensure_ascii=False)}")
        logger.debug(f"  - Found {len(similar_nodes)} Similar Existing Nodes for review.")

        for sim_node in similar_nodes:
            logger.debug(f"  - Reviewing existing node: '{sim_node['name']}' with score {sim_node['score']:.4f}")
            
            if sim_node['score'] >= config.DEDUPE_MERGE_THRESHOLD:
                node.persistent_uuid = sim_node['uuid']
                node.properties.name = sim_node['name']
                logger.debug(f"  - Verdict: MERGE (High-Confidence)")
                logger.debug(f"    - Score {sim_node['score']:.4f} >= {config.DEDUPE_MERGE_THRESHOLD}")
                logger.debug(f"    - Merged with: {self._clean_for_logging(sim_node)}")
                return node

            logger.debug(f"  - Verdict: ADJUDICATING (Medium-Confidence)")
            
            cleaned_sim_node_for_llm = self._clean_for_logging(sim_node)
            # æ¸…ç†å€™é€‰èŠ‚ç‚¹çš„ embeddingï¼Œé¿å…ä¼ é€’ç»™ LLM
            cleaned_candidate_node = self._clean_for_logging(node.model_dump())
            # å°†æ¸…ç†åçš„å­—å…¸è½¬æ¢å› Node å¯¹è±¡ï¼ˆç”¨äº model_dump_jsonï¼‰
            from schemas import NodeProperties
            cleaned_node = Node(
                temp_id=node.temp_id,
                label=node.label,
                persistent_uuid=node.persistent_uuid,
                properties=NodeProperties(**cleaned_candidate_node.get('properties', {}))
            )
            
            decision_response = await self.llm.decide_node_dedupe(
                memory_text,
                cleaned_sim_node_for_llm,
                cleaned_node
            )
            
            if decision_response and decision_response.decision == "MERGE":
                node.persistent_uuid = decision_response.merge_target_uuid or sim_node['uuid']
                # [å…³é”®ä¿®å¤] ä½¿ç”¨æ•°æ®åº“ä¸­å·²æœ‰çš„è§„èŒƒåç§°
                node.properties.name = sim_node['name'] 
                logger.debug(f"  - Final Verdict: MERGE (LLM Decision)")
                logger.debug(f"    - Merged with UUID: {node.persistent_uuid}")
                logger.debug(f"    - Node name normalized to: '{sim_node['name']}'")
                return node
            else:
                reason = decision_response.reason if decision_response else "LLM Adjudication failed"
                logger.debug(f"  - Verdict: CONTINUE (LLM decided NEW for this candidate. Reason: {reason})")
        
        # å¦‚æœå¾ªç¯èµ°å®Œéƒ½æ²¡æœ‰æ‰¾åˆ°å¯ä»¥åˆå¹¶çš„èŠ‚ç‚¹
        node.persistent_uuid = str(uuid.uuid4())
        logger.debug(f"  - Final Verdict: NEW (All candidates reviewed, no merge decision)")
        return node

    async def _resolve_relations(
    self,
    user_id: str,
    memory_id: str,
    memory_text: str,
    relations_from_llm: List[Relation],
    temp_id_map: Dict[str, str],
    resolved_nodes_list: List[Node]
) -> List[Relation]:
        """
        [*** é‡æ„ç‰ˆ-æœ€ç»ˆç‰ˆ ***]
        - ä½¿ç”¨â€œæ³›å‹FACTå…³ç³»â€æ¨¡å‹ã€‚
        - ç§»é™¤ LLM å†²çªè§£å†³ (æç¤ºè¯4)ã€‚
        - ä»…ä¾èµ–å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œå»é‡ (DISCARD or CREATE)ã€‚
        """
        nodes_by_uuid = {n.persistent_uuid: n for n in resolved_nodes_list if n.persistent_uuid}
        
        # æœ€ç»ˆä¿å­˜çš„å…³ç³»åˆ—è¡¨
        relations_to_save: List[Relation] = []
        
        # 1. å‡†å¤‡æ‰¹é‡åµŒå…¥
        facts_to_embed = []
        relations_to_process = []
        
        for rel in relations_from_llm:
            source_uuid = temp_id_map.get(rel.source_temp_id)
            target_uuid = temp_id_map.get(rel.target_temp_id)
            if not source_uuid or not target_uuid: 
                continue
            
            # [!!!] å…³é”®ï¼šç¡®ä¿ç±»å‹æ˜¯ "FACT"
            # (LLM åº”è¯¥è¿”å› "FACT", ä½†æˆ‘ä»¬æœ€å¥½è¿˜æ˜¯å¼ºåˆ¶è¦†ç›–å®ƒä»¥ç¡®ä¿å®‰å…¨)
            rel.type = "FACT" 
            
            rel.source_persistent_uuid = source_uuid
            rel.target_persistent_uuid = target_uuid
            rel.properties.source_memory_id = memory_id
            rel.properties.user_id = user_id

            # (å¥å£®çš„åç§°æŸ¥æ‰¾)
            source_node = nodes_by_uuid.get(source_uuid)
            target_node = nodes_by_uuid.get(target_uuid)
            # [!!] ä¿®å¤ï¼šUser èŠ‚ç‚¹å¯èƒ½ä¸åœ¨ resolved_nodes_list ä¸­
            source_name = "User" if rel.source_temp_id == "user" else (source_node.properties.name if source_node else "Unknown")
            target_name = "User" if rel.target_temp_id == "user" else (target_node.properties.name if target_node else "Unknown")
            
            # [!!!] æ–°çš„ fact_text æ„å»º, e.g., "(User)-[LIKES]->(Gaming)"
            fact_text = f"({source_name})-[{rel.properties.invented_type}]->({target_name})"
            
            rel.properties.fact = fact_text
            facts_to_embed.append(fact_text)
            relations_to_process.append(rel)
            
        if not relations_to_process: 
            return []
            
        relation_embeddings = self.embed.embed_batch(facts_to_embed)

        # 2. [æ–°] å‘é‡å»é‡é€»è¾‘
        for candidate_rel, embedding in zip(relations_to_process, relation_embeddings):
            candidate_rel.properties.fact_embedding = embedding
            
            logger.debug(f"--- ğŸ•µï¸ Relation Dedupe Check for: '{candidate_rel.properties.fact}' ---")

            # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨ *éå¸¸ç›¸ä¼¼* çš„äº‹å®
            similar_rels = await self.db.find_similar_relations(
                candidate_rel.source_persistent_uuid, 
                candidate_rel.target_persistent_uuid, 
                "FACT",  # <--- [!!!] ç¡¬ç¼–ç ä¸º "FACT"
                candidate_rel.properties.fact_embedding,
                top_k=1 
            )

            # [!!!] å…³é”®ï¼šæˆ‘ä»¬åªå…³å¿ƒåˆ†æ•°æœ€é«˜çš„é‚£ä¸ªæ˜¯å¦æ„æˆé‡å¤
            if similar_rels and similar_rels[0]['score'] >= config.DEDUPE_MERGE_THRESHOLD:
                # å·²ç»å­˜åœ¨ä¸€ä¸ªå‡ ä¹ä¸€æ ·çš„å…³ç³»ï¼Œä¸¢å¼ƒè¿™ä¸ªå€™é€‰ã€‚
                logger.debug(f"  - Verdict: DISCARD (Similar fact found in DB with score {similar_rels[0]['score']:.4f})")
                continue
            
            # è¿™æ˜¯ä¸€ä¸ªæ–°äº‹å®
            logger.debug(f"  - Verdict: NEW (No duplicate found in DB)")
            relations_to_save.append(candidate_rel)

        return relations_to_save