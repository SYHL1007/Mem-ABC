#llm_sevice.py
from pydantic import ValidationError, BaseModel
import json
import config
import prompts
import schemas
from utils import retry_async_llm_call
from typing import Dict, Any, Optional, Type
import logging
from openai import AsyncOpenAI, APIConnectionError
import asyncio
import json5
import re

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self):
        self.client = AsyncOpenAI(
            api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_BASE_URL,
            timeout=config.LLM_TIMEOUT,
            
        )
        logger.info(f"LLMService åˆå§‹åŒ–, base_url: {config.OPENAI_BASE_URL}")
        # [æ–°] è®°å½•ä½¿ç”¨çš„æ¨¡å‹
        logger.info(f"  > æå–æ¨¡å‹ (Fast): {config.LLM_MODEL_EXTRACT}")
        logger.info(f"  > æ¨ç†æ¨¡å‹ (Smart): {config.LLM_MODEL_REASON}")

    @retry_async_llm_call
    async def _call_api_json(
        self,
        system_prompt: str,
        user_prompt: str,
        response_model: Type[BaseModel],
        model_name: str  # <--- [æ–°] æ¥å—æ¨¡å‹åç§°
    ) -> Optional[BaseModel]:
        """
        è°ƒç”¨ LLM API å¹¶æœŸæœ›è¿”å› Pydantic æ¨¡å‹æ ¡éªŒè¿‡çš„ JSONã€‚
        """
        content = ""
        try:
            completion = await self.client.chat.completions.create(
                model=model_name,  # <--- [ä¿®æ”¹] ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
            )
            content = completion.choices[0].message.content
            
            validated_data = response_model.model_validate_json(content)
            return validated_data

        except (ValidationError, json.JSONDecodeError) as e:
            logger.warning(
                f"LLM è¾“å‡º Pydantic/JSON æ ¡éªŒå¤±è´¥ (æ¨¡å‹: {model_name}), å°†è§¦å‘é‡è¯•... Error: {e}\nRaw Content: {content}"
            )
            raise APIConnectionError(request=None, message="Retrying due to validation error.")

        except Exception as e:
            logger.error(f"LLM API (æ¨¡å‹: {model_name}) è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            raise 

    async def extract_nodes(self, text: str) -> Optional[schemas.NodeExtractionResponse]:
        """è°ƒç”¨æç¤ºè¯1ï¼šèŠ‚ç‚¹æŠ½å– (ä½¿ç”¨ Fast æ¨¡å‹)"""
        logger.debug(f"å¼€å§‹æŠ½å–èŠ‚ç‚¹ (Fast Model) for text: {text[:50]}...")
        system_prompt = prompts.PROMPT_NODE_EXTRACT["system"]
        user_prompt = prompts.PROMPT_NODE_EXTRACT["user"].format(input_text=text)
        
        response = await self._call_api_json(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_model=schemas.NodeExtractionResponse,
            model_name=config.LLM_MODEL_EXTRACT  # <--- [ä¿®æ”¹]
        )
        if response:
            logger.debug(f"--- ğŸŸ¢ LLM Extracted Nodes ---\n{json.dumps(response.model_dump(), indent=2, ensure_ascii=False)}")
        return response

    async def extract_relations(self, text: str, nodes_json: str) -> Optional[schemas.RelationExtractionResponse]:
        """è°ƒç”¨æç¤ºè¯2ï¼šå…³ç³»æŠ½å– (ä½¿ç”¨ Fast æ¨¡å‹)"""
        logger.debug(f"å¼€å§‹æŠ½å–å…³ç³» (Fast Model) for text: {text[:50]}...")
        system_prompt = prompts.PROMPT_RELATION_EXTRACT["system"]
        user_prompt = prompts.PROMPT_RELATION_EXTRACT["user"].format(
            input_text=text,
            node_list_json=nodes_json
        )
        
        response = await self._call_api_json(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_model=schemas.RelationExtractionResponse,
            model_name=config.LLM_MODEL_EXTRACT  # <--- [ä¿®æ”¹]
        )
        if response:
            logger.debug(f"--- ğŸŸ¢ LLM Extracted Relations ---\n{json.dumps(response.model_dump(), indent=2, ensure_ascii=False)}")
        return response

    async def decide_node_dedupe(
        self,
        input_text: str,
        existing_node: Dict[str, Any],
        candidate_node: schemas.Node
    ) -> Optional[schemas.NodeDedupeDecision]:
        """è°ƒç”¨æç¤ºè¯3ï¼šèŠ‚ç‚¹å»é‡ä»²è£ (ä½¿ç”¨ Smart æ¨¡å‹)"""
        logger.debug(f"LLM ä»²è£èŠ‚ç‚¹ (Smart Model): {candidate_node.properties.name}")
        user_prompt = prompts.PROMPT_NODE_DEDUPE["user"].format(
            input_text=input_text,
            existing_node=json.dumps(existing_node),
            candidate_node=candidate_node.model_dump_json()
        )
        system_prompt = prompts.PROMPT_NODE_DEDUPE["system"]
        
        response = await self._call_api_json(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_model=schemas.NodeDedupeDecision,
            model_name=config.LLM_MODEL_EXTRACT  # <--- [ä¿®æ”¹]
        )
        if response:
            logger.debug(f"--- ğŸ”µ Node Dedupe Decision ---")
            logger.debug(f" Â - Candidate: {candidate_node.properties.name}")
            logger.debug(f" Â - Decision: {response.decision}")
            if response.decision == "MERGE":
                logger.debug(f" Â - Target: {response.merge_target_uuid}")
            logger.debug(f" Â - Reason: {response.reason}")
            logger.debug("----------------------------------")
        return response

    # --- [æ–°] ç”¨äºæç¤ºè¯6çš„å‡½æ•° (ä½¿ç”¨ Smart æ¨¡å‹) ---
    async def synthesize_profile(self, context: str) -> Optional[schemas.ProfileSynthesisResponse]:
        """è°ƒç”¨æç¤ºè¯6: ä»KGä¸Šä¸‹æ–‡åˆæˆç”¨æˆ·ç”»åƒ (ä½¿ç”¨ Smart æ¨¡å‹)"""
        logger.debug(f"åˆæˆç”¨æˆ·ç”»åƒ (Smart Model)...")
        system_prompt = prompts.PROMPT_PROFILE_SYNTHESIZE["system"]
        user_prompt = prompts.PROMPT_PROFILE_SYNTHESIZE["user"].format(context=context)
        
        response = await self._call_api_json(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_model=schemas.ProfileSynthesisResponse,
            model_name=config.LLM_MODEL_REASON  # <--- [ä¿®æ”¹]
        )
        return response


    async def parse_personalized_answer(self, raw_output: str, max_retries: int = 10):
        """
        è§£æ LLM è¿”å›çš„ JSON/JSON5ï¼Œå¹¶ä»…æå– personalized_answerã€‚
        è‡ªåŠ¨æ¸…ç† Markdown ä»£ç å—ï¼Œè‡ªåŠ¨é‡è¯•ã€‚
        """

        import asyncio
        import json5
        import re

        # fix max_retries
        try:
            max_retries = int(max_retries)
        except:
            max_retries = 10

        attempt = 0

        while attempt < max_retries:

            try:
                # ---- Step 0: ç±»å‹æ£€æŸ¥ ----
                if not isinstance(raw_output, str):
                    raise TypeError(f"raw_output ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè€Œæ˜¯ {type(raw_output)}")

                # ---- Step 1: ä½¿ç”¨æ­£åˆ™æå– JSON å†…å®¹ (æœ€ç¨³å¥çš„æ–¹å¼) ----
                # é€»è¾‘ï¼šå¯»æ‰¾è¢« ```json ... ``` åŒ…è£¹çš„å†…å®¹ï¼Œæˆ–è€…å¯»æ‰¾æœ€å¤–å±‚çš„ { ... }
                # 1. å°è¯•åŒ¹é… Markdown ä»£ç å—
                pattern = r"```(?:json)?\s*(\{.*?\})\s*```"
                match = re.search(pattern, raw_output, re.DOTALL | re.IGNORECASE)
                
                if match:
                    json_str = match.group(1)
                else:
                    # 2. å¦‚æœæ²¡æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•å¯»æ‰¾é¦–å°¾çš„å¤§æ‹¬å· (å…œåº•ç­–ç•¥)
                    start = raw_output.find("{")
                    end = raw_output.rfind("}")
                    if start != -1 and end != -1:
                        json_str = raw_output[start : end + 1]
                    else:
                        # å®åœ¨æ‰¾ä¸åˆ°ï¼Œå°±æ­»é©¬å½“æ´»é©¬åŒ»ï¼Œç”¨åŸå§‹å­—ç¬¦ä¸²
                        json_str = raw_output

                # ---- Step 2: è§£æ JSON5 ----
                # æ³¨æ„ï¼šåƒä¸‡ä¸è¦æ‰‹åŠ¨ replace("\n", "")ï¼Œjson5 ä¼šè‡ªå·±å¤„ç†æ¢è¡Œ
                data = json5.loads(json_str)

                # ---- Step 3: ç±»å‹ä¸é”®æ£€æŸ¥ (å›ç­”ä½ çš„é—®é¢˜) ----
                if not isinstance(data, dict):
                    raise ValueError(f"è§£æå‡ºçš„æ•°æ®ä¸æ˜¯å­—å…¸ï¼Œè€Œæ˜¯ {type(data)}")

                if "personalized_answer" not in data:
                    raise ValueError("JSON ä¸­ç¼ºå°‘ 'personalized_answer' å­—æ®µ")

                return data["personalized_answer"]


            except Exception as e:
                attempt += 1

                if attempt >= max_retries:
                    raise RuntimeError(
                        f"JSON5 è§£æå¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {e}\nåŸå§‹è¾“å‡º:\n{raw_output}"
                    )

                # å°çš„ç­‰å¾…é¿å…ç–¯ç‹‚é‡è¯•
                await asyncio.sleep(0.1)

    async def generate_answer_from_context(self, question: str, context: str,json:bool=True) -> str:
        """ä½¿ç”¨ Smart Model ç”Ÿæˆå›ç­”ï¼Œå¹¶è§£æ JSON5 ä»…è¿”å› personalized_answer"""
        logger.debug(f"æ ‡å‡†QA (Smart Model): {question[:50]}...") 
        if json:  
            user_prompt = prompts.PROMPT_QA_JSON["user"].format(context=context, question=question)
            system_prompt = prompts.PROMPT_QA_JSON["system"]
        else: 
            user_prompt = prompts.PROMPT_QA["user"].format(context=context, question=question)
            system_prompt = prompts.PROMPT_QA["system"]
        
        try:
            completion = await self.client.chat.completions.create(
                model=config.LLM_MODEL_REASON,  
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1
            )

            raw_output = completion.choices[0].message.content
            if json:
            # ğŸ”¥ æ ¸å¿ƒï¼šè§£æ JSON5 å¹¶åªè¿”å› personalized_answer
                return await self.parse_personalized_answer(raw_output)
            else:
                return raw_output

        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆ(æ ‡å‡†ç‰ˆ)å¤±è´¥: {e}", exc_info=True)
            return "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ã€‚"


    # # --- [ä¿®æ”¹] ç”¨äºæç¤ºè¯5çš„å‡½æ•° (ä½¿ç”¨ Smart æ¨¡å‹) ---
    # @retry_async_llm_call
    # async def generate_answer_from_context(self, question: str, context: str) -> str:
    #     """è°ƒç”¨æç¤ºè¯5 (æ ‡å‡†ç‰ˆ): ä»åŸå§‹KGä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ (ä½¿ç”¨ Smart æ¨¡å‹)"""
    #     logger.debug(f"æ ‡å‡†QA (Smart Model): {question[:50]}...")    
    #     user_prompt = prompts.PROMPT_QA["user"].format(context=context, question=question)
    #     system_prompt = prompts.PROMPT_QA["system"]
        
    #     try:
    #         completion = await self.client.chat.completions.create(
    #             model=config.LLM_MODEL_REASON,  
    #             messages=[
    #                 {"role": "system", "content": system_prompt},
    #                 {"role": "user", "content": user_prompt}
    #             ],
    #             temperature=0.1
    #         )
    #         return completion.choices[0].message.content
    #     except Exception as e:
    #         logger.error(f"ç”Ÿæˆç­”æ¡ˆ(æ ‡å‡†ç‰ˆ)å¤±è´¥: {e}", exc_info=True)
    #         return "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ã€‚"

    # --- [æ–°] ç”¨äºæç¤ºè¯7çš„å‡½æ•° ---
    @retry_async_llm_call
    async def generate_answer_from_profile(self, question: str, profile: str) -> str:
        """è°ƒç”¨æç¤ºè¯7 (å¢å¼ºç‰ˆ): ä»åˆæˆçš„ç”»åƒç”Ÿæˆç­”æ¡ˆ (ä½¿ç”¨ Smart æ¨¡å‹)"""
        logger.debug(f"å¢å¼ºQA (Smart Model): {question[:50]}...")
        user_prompt = prompts.PROMPT_QA_WITH_PROFILE["user"].format(profile=profile, question=question)
        system_prompt = prompts.PROMPT_QA_WITH_PROFILE["system"]
        
        try:
            completion = await self.client.chat.completions.create(
                model=config.LLM_MODEL_REASON, 
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
            return completion.choices[0].message.content
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆ(å¢å¼ºç‰ˆ)å¤±è´¥: {e}", exc_info=True)
            return "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ã€‚"


# å•ä¾‹
llm_service_instance = LLMService()